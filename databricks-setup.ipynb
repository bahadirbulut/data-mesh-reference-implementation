{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bf468c1",
   "metadata": {},
   "source": [
    "## 1. Environment Check\n",
    "\n",
    "Verify that Spark and Delta Lake are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Spark version\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Scala Version: {spark.sparkContext._conf.get('spark.executor.extraJavaOptions', 'N/A')}\")\n",
    "\n",
    "# Check Delta Lake\n",
    "try:\n",
    "    import delta\n",
    "    print(f\"Delta Lake Version: {delta.__version__}\")\n",
    "    print(\"✓ Delta Lake is available\")\n",
    "except ImportError:\n",
    "    print(\"✗ Delta Lake not found - install or use runtime 11.0+\")\n",
    "\n",
    "# List available databases\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "print(\"\\n✓ Environment check complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661a97d",
   "metadata": {},
   "source": [
    "## 2. Upload Sample Data to DBFS\n",
    "\n",
    "**Option A: Manual Upload (Recommended for Community Edition)**\n",
    "1. Go to \"Data\" tab in Databricks\n",
    "2. Click \"Create Table\"\n",
    "3. Upload files:\n",
    "   - `domains/sales/sample_data/orders_2025-01-01.csv`\n",
    "   - `domains/research_ops/sample_data/experiments_2025-01-01.csv`\n",
    "4. Note the DBFS paths (e.g., `/FileStore/tables/orders_2025_01_01.csv`)\n",
    "\n",
    "**Option B: Programmatic Upload (if files are in Workspace)**\n",
    "Run the cell below if you've uploaded files to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce921b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Copy from workspace to DBFS (adjust paths as needed)\n",
    "# Uncomment if using this approach\n",
    "\n",
    "# import os\n",
    "# \n",
    "# # Create raw data directories\n",
    "# dbutils.fs.mkdirs(\"dbfs:/raw/sales/\")\n",
    "# dbutils.fs.mkdirs(\"dbfs:/raw/research_ops/\")\n",
    "# \n",
    "# # Copy sales data\n",
    "# dbutils.fs.cp(\n",
    "#     \"file:/Workspace/Users/<your_email>/orders_2025-01-01.csv\",\n",
    "#     \"dbfs:/raw/sales/orders_2025-01-01.csv\"\n",
    "# )\n",
    "# \n",
    "# # Copy research ops data\n",
    "# dbutils.fs.cp(\n",
    "#     \"file:/Workspace/Users/<your_email>/experiments_2025-01-01.csv\",\n",
    "#     \"dbfs:/raw/research_ops/experiments_2025-01-01.csv\"\n",
    "# )\n",
    "# \n",
    "# print(\"✓ Sample data uploaded to DBFS\")\n",
    "\n",
    "# For manual upload, just set the paths here:\n",
    "sales_raw_path = \"dbfs:/FileStore/tables/orders_2025_01_01.csv\"  # Adjust based on actual upload\n",
    "experiments_raw_path = \"dbfs:/FileStore/tables/experiments_2025_01_01.csv\"  # Adjust based on actual upload\n",
    "\n",
    "print(f\"Sales data path: {sales_raw_path}\")\n",
    "print(f\"Experiments data path: {experiments_raw_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d94e6",
   "metadata": {},
   "source": [
    "## 3. Sales Domain - Bronze Layer\n",
    "\n",
    "Read raw CSV data and write to Bronze layer with metadata.\n",
    "\n",
    "**Bronze Layer Purpose:**\n",
    "- Ingest raw data as-is\n",
    "- Add metadata (_ingestion_timestamp, _source_file, _ingestion_date)\n",
    "- Store in Delta format for ACID guarantees\n",
    "- Partition by ingestion date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3324a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name, current_timestamp, to_date, lit\n",
    "\n",
    "# Configuration\n",
    "bronze_path = \"dbfs:/bronze/sales/orders/\"\n",
    "\n",
    "# Read raw CSV\n",
    "print(\"Reading raw sales data...\")\n",
    "sales_df = spark.read.csv(\n",
    "    sales_raw_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(f\"Read {sales_df.count()} rows\")\n",
    "print(\"\\nSchema:\")\n",
    "sales_df.printSchema()\n",
    "\n",
    "# Add metadata columns\n",
    "sales_bronze_df = sales_df \\\n",
    "    .withColumn(\"_ingestion_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"_source_file\", lit(sales_raw_path)) \\\n",
    "    .withColumn(\"_ingestion_date\", to_date(current_timestamp()))\n",
    "\n",
    "# Write to Delta (Bronze layer)\n",
    "print(\"\\nWriting to Bronze layer...\")\n",
    "sales_bronze_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"_ingestion_date\") \\\n",
    "    .save(bronze_path)\n",
    "\n",
    "print(f\"✓ Bronze: Wrote {sales_bronze_df.count()} rows to {bronze_path}\")\n",
    "\n",
    "# Verify\n",
    "print(\"\\nSample data:\")\n",
    "spark.read.format(\"delta\").load(bronze_path).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8a4d4",
   "metadata": {},
   "source": [
    "## 4. Sales Domain - Silver Layer\n",
    "\n",
    "Clean and validate data from Bronze layer.\n",
    "\n",
    "**Silver Layer Purpose:**\n",
    "- Deduplicate records\n",
    "- Validate data quality (quantity > 0, unit_price >= 0)\n",
    "- Type conversions and standardization\n",
    "- Remove invalid records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Configuration\n",
    "silver_path = \"dbfs:/silver/sales/orders/\"\n",
    "\n",
    "# Read from Bronze\n",
    "print(\"Reading from Bronze layer...\")\n",
    "bronze_df = spark.read.format(\"delta\").load(bronze_path)\n",
    "print(f\"Bronze records: {bronze_df.count()}\")\n",
    "\n",
    "# Deduplicate by order_id (keep latest by ingestion timestamp)\n",
    "print(\"\\nDeduplicating...\")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"order_id\").orderBy(col(\"_ingestion_timestamp\").desc())\n",
    "silver_df = bronze_df \\\n",
    "    .withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"row_num\") == 1) \\\n",
    "    .drop(\"row_num\")\n",
    "\n",
    "# Data quality checks\n",
    "print(\"Applying quality checks...\")\n",
    "silver_df = silver_df.filter(\n",
    "    (col(\"quantity\").isNotNull()) &\n",
    "    (col(\"quantity\") > 0) &\n",
    "    (col(\"unit_price\").isNotNull()) &\n",
    "    (col(\"unit_price\") >= 0)\n",
    ")\n",
    "\n",
    "# Type conversions\n",
    "silver_df = silver_df \\\n",
    "    .withColumn(\"quantity\", col(\"quantity\").cast(\"int\")) \\\n",
    "    .withColumn(\"unit_price\", col(\"unit_price\").cast(\"decimal(10,2)\"))\n",
    "\n",
    "# Write to Delta (Silver layer)\n",
    "print(\"\\nWriting to Silver layer...\")\n",
    "silver_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(silver_path)\n",
    "\n",
    "print(f\"✓ Silver: Wrote {silver_df.count()} validated rows to {silver_path}\")\n",
    "\n",
    "# Verify\n",
    "print(\"\\nSample data:\")\n",
    "spark.read.format(\"delta\").load(silver_path).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd4b8d9",
   "metadata": {},
   "source": [
    "## 5. Sales Domain - Gold Layer\n",
    "\n",
    "Create aggregated data product: daily revenue.\n",
    "\n",
    "**Gold Layer Purpose:**\n",
    "- Business-level aggregations\n",
    "- Ready for analytics and reporting\n",
    "- Conforms to data contracts\n",
    "- Optimized for consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "# Configuration\n",
    "gold_path = \"dbfs:/gold/sales/daily_revenue/\"\n",
    "\n",
    "# Read from Silver\n",
    "print(\"Reading from Silver layer...\")\n",
    "silver_df = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "# Calculate revenue for each order\n",
    "print(\"\\nCalculating revenue...\")\n",
    "gold_df = silver_df.withColumn(\n",
    "    \"revenue\",\n",
    "    col(\"quantity\") * col(\"unit_price\")\n",
    ")\n",
    "\n",
    "# Aggregate by order_date\n",
    "print(\"Aggregating by date...\")\n",
    "gold_df = gold_df.groupBy(\"order_date\").agg(\n",
    "    _sum(\"revenue\").alias(\"daily_revenue\")\n",
    ").orderBy(\"order_date\")\n",
    "\n",
    "# Write to Delta (Gold layer)\n",
    "print(\"\\nWriting to Gold layer...\")\n",
    "gold_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(gold_path)\n",
    "\n",
    "print(f\"✓ Gold: Wrote {gold_df.count()} aggregated rows to {gold_path}\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nDaily Revenue Report:\")\n",
    "gold_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd7588e",
   "metadata": {},
   "source": [
    "## 6. Research Ops Domain - Bronze Layer\n",
    "\n",
    "Ingest experiment data with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99590322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "bronze_path = \"dbfs:/bronze/research_ops/experiments/\"\n",
    "\n",
    "# Read raw CSV\n",
    "print(\"Reading raw experiments data...\")\n",
    "exp_df = spark.read.csv(\n",
    "    experiments_raw_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(f\"Read {exp_df.count()} rows\")\n",
    "print(\"\\nSchema:\")\n",
    "exp_df.printSchema()\n",
    "\n",
    "# Add metadata\n",
    "exp_bronze_df = exp_df \\\n",
    "    .withColumn(\"_ingestion_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"_source_file\", lit(experiments_raw_path)) \\\n",
    "    .withColumn(\"_ingestion_date\", to_date(current_timestamp()))\n",
    "\n",
    "# Write to Bronze\n",
    "print(\"\\nWriting to Bronze layer...\")\n",
    "exp_bronze_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"_ingestion_date\") \\\n",
    "    .save(bronze_path)\n",
    "\n",
    "print(f\"✓ Bronze: Wrote {exp_bronze_df.count()} rows to {bronze_path}\")\n",
    "\n",
    "# Verify\n",
    "print(\"\\nSample data:\")\n",
    "spark.read.format(\"delta\").load(bronze_path).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1445deb7",
   "metadata": {},
   "source": [
    "## 7. Research Ops Domain - Silver Layer\n",
    "\n",
    "Clean and validate experiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e83c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper, trim\n",
    "\n",
    "# Configuration\n",
    "silver_path = \"dbfs:/silver/research_ops/experiments/\"\n",
    "\n",
    "# Read from Bronze\n",
    "print(\"Reading from Bronze layer...\")\n",
    "bronze_df = spark.read.format(\"delta\").load(bronze_path)\n",
    "print(f\"Bronze records: {bronze_df.count()}\")\n",
    "\n",
    "# Deduplicate by experiment_id\n",
    "print(\"\\nDeduplicating...\")\n",
    "window_spec = Window.partitionBy(\"experiment_id\").orderBy(col(\"_ingestion_timestamp\").desc())\n",
    "silver_df = bronze_df \\\n",
    "    .withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"row_num\") == 1) \\\n",
    "    .drop(\"row_num\")\n",
    "\n",
    "# Standardize IDs (uppercase, trim)\n",
    "print(\"Standardizing data...\")\n",
    "silver_df = silver_df \\\n",
    "    .withColumn(\"experiment_id\", upper(trim(col(\"experiment_id\")))) \\\n",
    "    .withColumn(\"researcher_id\", upper(trim(col(\"researcher_id\"))))\n",
    "\n",
    "# Data quality checks\n",
    "print(\"Applying quality checks...\")\n",
    "silver_df = silver_df.filter(\n",
    "    (col(\"trial_count\").isNotNull()) &\n",
    "    (col(\"trial_count\") > 0) &\n",
    "    (col(\"duration_minutes\").isNotNull()) &\n",
    "    (col(\"duration_minutes\") >= 0)\n",
    ")\n",
    "\n",
    "# Type conversions\n",
    "silver_df = silver_df \\\n",
    "    .withColumn(\"trial_count\", col(\"trial_count\").cast(\"int\")) \\\n",
    "    .withColumn(\"duration_minutes\", col(\"duration_minutes\").cast(\"decimal(10,2)\"))\n",
    "\n",
    "# Write to Silver\n",
    "print(\"\\nWriting to Silver layer...\")\n",
    "silver_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(silver_path)\n",
    "\n",
    "print(f\"✓ Silver: Wrote {silver_df.count()} validated rows to {silver_path}\")\n",
    "\n",
    "# Verify\n",
    "print(\"\\nSample data:\")\n",
    "spark.read.format(\"delta\").load(silver_path).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1553070d",
   "metadata": {},
   "source": [
    "## 8. Research Ops Domain - Gold Layer\n",
    "\n",
    "Create aggregated experiment metrics by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c553e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, avg\n",
    "\n",
    "# Configuration\n",
    "gold_path = \"dbfs:/gold/research_ops/daily_metrics/\"\n",
    "\n",
    "# Read from Silver\n",
    "print(\"Reading from Silver layer...\")\n",
    "silver_df = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "# Aggregate by experiment_date\n",
    "print(\"\\nCalculating metrics...\")\n",
    "gold_df = silver_df.groupBy(\"experiment_date\").agg(\n",
    "    _sum(\"trial_count\").alias(\"total_trials\"),\n",
    "    count(\"experiment_id\").alias(\"experiment_count\"),\n",
    "    avg(\"duration_minutes\").alias(\"avg_duration_minutes\")\n",
    ").orderBy(\"experiment_date\")\n",
    "\n",
    "# Write to Gold\n",
    "print(\"\\nWriting to Gold layer...\")\n",
    "gold_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(gold_path)\n",
    "\n",
    "print(f\"✓ Gold: Wrote {gold_df.count()} aggregated rows to {gold_path}\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nDaily Experiment Metrics:\")\n",
    "gold_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf594b8",
   "metadata": {},
   "source": [
    "## 9. Verification & Queries\n",
    "\n",
    "Run SQL queries to verify the pipeline results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7262ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Delta tables as temp views\n",
    "print(\"Registering tables...\\n\")\n",
    "\n",
    "spark.read.format(\"delta\").load(\"dbfs:/gold/sales/daily_revenue/\") \\\n",
    "    .createOrReplaceTempView(\"sales_daily_revenue\")\n",
    "\n",
    "spark.read.format(\"delta\").load(\"dbfs:/gold/research_ops/daily_metrics/\") \\\n",
    "    .createOrReplaceTempView(\"research_ops_daily_metrics\")\n",
    "\n",
    "print(\"✓ Tables registered\\n\")\n",
    "\n",
    "# Query 1: Sales revenue summary\n",
    "print(\"=\" * 60)\n",
    "print(\"Query 1: Sales Revenue Summary\")\n",
    "print(\"=\" * 60)\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    order_date,\n",
    "    ROUND(daily_revenue, 2) as daily_revenue\n",
    "FROM sales_daily_revenue\n",
    "ORDER BY order_date\n",
    "\"\"\").show()\n",
    "\n",
    "# Query 2: Total revenue\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Query 2: Total Revenue\")\n",
    "print(\"=\" * 60)\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_days,\n",
    "    ROUND(SUM(daily_revenue), 2) as total_revenue,\n",
    "    ROUND(AVG(daily_revenue), 2) as avg_daily_revenue\n",
    "FROM sales_daily_revenue\n",
    "\"\"\").show()\n",
    "\n",
    "# Query 3: Research ops metrics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Query 3: Research Ops Metrics\")\n",
    "print(\"=\" * 60)\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    experiment_date,\n",
    "    experiment_count,\n",
    "    total_trials,\n",
    "    ROUND(avg_duration_minutes, 2) as avg_duration_minutes\n",
    "FROM research_ops_daily_metrics\n",
    "ORDER BY experiment_date\n",
    "\"\"\").show()\n",
    "\n",
    "# Query 4: Data quality check\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Query 4: Data Quality - Row Counts by Layer\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sales_bronze_count = spark.read.format(\"delta\").load(\"dbfs:/bronze/sales/orders/\").count()\n",
    "sales_silver_count = spark.read.format(\"delta\").load(\"dbfs:/silver/sales/orders/\").count()\n",
    "sales_gold_count = spark.read.format(\"delta\").load(\"dbfs:/gold/sales/daily_revenue/\").count()\n",
    "\n",
    "exp_bronze_count = spark.read.format(\"delta\").load(\"dbfs:/bronze/research_ops/experiments/\").count()\n",
    "exp_silver_count = spark.read.format(\"delta\").load(\"dbfs:/silver/research_ops/experiments/\").count()\n",
    "exp_gold_count = spark.read.format(\"delta\").load(\"dbfs:/gold/research_ops/daily_metrics/\").count()\n",
    "\n",
    "print(f\"Sales Domain:\")\n",
    "print(f\"  Bronze: {sales_bronze_count} rows\")\n",
    "print(f\"  Silver: {sales_silver_count} rows (deduped/validated)\")\n",
    "print(f\"  Gold:   {sales_gold_count} rows (aggregated)\")\n",
    "print(f\"\\nResearch Ops Domain:\")\n",
    "print(f\"  Bronze: {exp_bronze_count} rows\")\n",
    "print(f\"  Silver: {exp_silver_count} rows (deduped/validated)\")\n",
    "print(f\"  Gold:   {exp_gold_count} rows (aggregated)\")\n",
    "\n",
    "print(\"\\n✓ All queries complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758872f4",
   "metadata": {},
   "source": [
    "## 10. View Delta Table History\n",
    "\n",
    "Delta Lake provides time travel and audit capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# View history of sales gold table\n",
    "print(\"Sales Daily Revenue - Table History:\")\n",
    "print(\"=\" * 80)\n",
    "dt = DeltaTable.forPath(spark, \"dbfs:/gold/sales/daily_revenue/\")\n",
    "dt.history().select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False)\n",
    "\n",
    "# View history of research ops gold table\n",
    "print(\"\\nResearch Ops Daily Metrics - Table History:\")\n",
    "print(\"=\" * 80)\n",
    "dt2 = DeltaTable.forPath(spark, \"dbfs:/gold/research_ops/daily_metrics/\")\n",
    "dt2.history().select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26234c20",
   "metadata": {},
   "source": [
    "## 11. Cleanup (Optional)\n",
    "\n",
    "Remove test data and Delta tables. **Run only when done testing!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This will delete all test data!\n",
    "# Uncomment to run cleanup\n",
    "\n",
    "# print(\"Cleaning up test data...\\n\")\n",
    "# \n",
    "# # Remove Delta tables\n",
    "# paths_to_remove = [\n",
    "#     \"dbfs:/bronze/sales/orders/\",\n",
    "#     \"dbfs:/silver/sales/orders/\",\n",
    "#     \"dbfs:/gold/sales/daily_revenue/\",\n",
    "#     \"dbfs:/bronze/research_ops/experiments/\",\n",
    "#     \"dbfs:/silver/research_ops/experiments/\",\n",
    "#     \"dbfs:/gold/research_ops/daily_metrics/\"\n",
    "# ]\n",
    "# \n",
    "# for path in paths_to_remove:\n",
    "#     try:\n",
    "#         dbutils.fs.rm(path, recurse=True)\n",
    "#         print(f\"✓ Removed: {path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"✗ Failed to remove {path}: {e}\")\n",
    "# \n",
    "# print(\"\\n✓ Cleanup complete\")\n",
    "\n",
    "print(\"Cleanup commands are commented out.\")\n",
    "print(\"Uncomment the code above to remove test data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3189242e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "✅ **Environment validation** - Checked Spark and Delta Lake\n",
    "\n",
    "✅ **Data ingestion** - Uploaded sample CSVs to DBFS\n",
    "\n",
    "✅ **Bronze layer** - Raw data with metadata\n",
    "\n",
    "✅ **Silver layer** - Deduplication and validation\n",
    "\n",
    "✅ **Gold layer** - Business aggregations\n",
    "\n",
    "✅ **Two domains** - Sales and Research Ops\n",
    "\n",
    "✅ **Verification** - SQL queries and row counts\n",
    "\n",
    "✅ **Time travel** - Delta Lake history\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Modify transformations** - Adjust Silver/Gold logic for your use case\n",
    "2. **Add more domains** - Create new domain folders and pipelines\n",
    "3. **Test with larger data** - Upload bigger CSV files\n",
    "4. **Create dashboards** - Use SQL queries in Databricks SQL\n",
    "5. **Schedule jobs** - Convert to Databricks Jobs for automation\n",
    "6. **Connect to ADLS** - For production, use Azure Storage\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [Databricks Documentation](https://docs.databricks.com/)\n",
    "- [Delta Lake Guide](https://docs.delta.io/)\n",
    "- [PySpark API](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Data Mesh Principles](https://www.datamesh-architecture.com/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
